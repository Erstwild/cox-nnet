{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nArtificial neural networks (ANN) are computing architectures with massively parallel interconnections of simple neurons. It has been applied to biomedical fields such as imaging analysis and diagnosis.  \n\n\nIn this package built on the Theano math library, we implement ANNs to predict patient prognosis by extending Cox Regression to the non-linear neural network framework.  \n\n\nSome features of Cox-nnet include parallelization and GPU usage for high computational efficiency, training optimization methods such as the Nesterov accelerated gradient and flexibility in allowing the specification of neural network architecture.  Incorporation of biological information into the structure or using a deep learning approach are possible.  \n\n\nIn all, we think that this new tool will tremendously help researchers to build stronger predictive models as well as investigate the importance of variables associated with patient outcome.  \n\n\nFunctions overview\n\n\n\n\ntrainCoxMlp\n - main function for training a Cox-nnet model\n\n\nCVLoglikelihood\n - calculate the cross-validated logliklihood (model performance metric)\n\n\nCIndex\n - calculate the C-Index (model performance metric)\n\n\nL2CVSearch\n - helper function for optimizing the regularization parameter; searches for lambda using a hill climbing algorithm\n\n\nL2CVProfile\n - helper function for optimizing the regularization parameter; profiles lambda across a range of values\n\n\nevalNewData\n - evaluates new data or test data; outputs the linear predictor (i.e., the log hazard ratio)\n\n\nvarImportance\n - determine variable importance through a dropout procedure\n\n\nsaveModel\n - saves model to binary file\n\n\nloadModel\n - loads model from file", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "Artificial neural networks (ANN) are computing architectures with massively parallel interconnections of simple neurons. It has been applied to biomedical fields such as imaging analysis and diagnosis.    In this package built on the Theano math library, we implement ANNs to predict patient prognosis by extending Cox Regression to the non-linear neural network framework.    Some features of Cox-nnet include parallelization and GPU usage for high computational efficiency, training optimization methods such as the Nesterov accelerated gradient and flexibility in allowing the specification of neural network architecture.  Incorporation of biological information into the structure or using a deep learning approach are possible.    In all, we think that this new tool will tremendously help researchers to build stronger predictive models as well as investigate the importance of variables associated with patient outcome.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#functions-overview", 
            "text": "trainCoxMlp  - main function for training a Cox-nnet model  CVLoglikelihood  - calculate the cross-validated logliklihood (model performance metric)  CIndex  - calculate the C-Index (model performance metric)  L2CVSearch  - helper function for optimizing the regularization parameter; searches for lambda using a hill climbing algorithm  L2CVProfile  - helper function for optimizing the regularization parameter; profiles lambda across a range of values  evalNewData  - evaluates new data or test data; outputs the linear predictor (i.e., the log hazard ratio)  varImportance  - determine variable importance through a dropout procedure  saveModel  - saves model to binary file  loadModel  - loads model from file", 
            "title": "Functions overview"
        }, 
        {
            "location": "/trainCoxMlp/", 
            "text": "trainCoxMlp\n\n\nThis function is the main entry point to building a Cox-nnet survival model.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient\n\n\nmodel_params = {}\n - model parameters, see below\n\n\nsearch_params = {}\n - model search/training parameters, see below\n\n\nverbose=True\n - print more stuff.  \n\n\n\n\nmodel_params\n is a dictionary of model parameters used in model training.  It has the following parameters:\n\n\n\n\nL2_reg\n - Ridge regression parameter for regularization.  Default value is \nnumpy.exp(-1)\n\n\nnode_map\n - mapping of neurons in network\n\n\ninput_split\n - splitting of input layer for customization of the network architecture\n\n\n\n\nThe network architecture is defined by the \nnode_map\n and \ninput_split\n parameters.  Setting these values to \nNone\n will default to a neural network of one hidden layer with the number of neurons in the hidden layer = \nceil( sqrt( # of input nodes ))\n.  \ninput_split\n is a list of lists with the indices of the inputs determining which sub-input layer each input belongs to.  For example, if you want to split an input with four features into two groups of two: \ninput_split = [[0,1], [2,3]]\n.  \n\n\nnode_map\n defines the connections of the input layer (or sub-input layers).  \nnode_map\n list of list of triples (one list for each hidden layer).  A triple defines the inputs of a sub-hidden layer and how many output nodes it has.  I.e., \n(# of input neurons, list of input sub-layers, # of output neurons)\n.  The \nsimpleNetArch\n function can be used to generate an architecture with a given number of hidden layer neurons.  \n\n\nsearch params\n is a dictionary of search/optimization hyper-parameters.  Generally the default values will work pretty well and you don't need to change these parameters.  \n\n\n\n\nmethod\n - The algorithm for gradient descent.  Includes standard gradient descent (\"gradient\"), Nesterov accelerated gradient \"nesterov\" and momentum gradient descent (\"momentum\").  Default is \"nesterov\".  \n\n\nlearning_rate\n - Initial learning rate.  Default is 0.01\n\n\nmomentum\n - proportion of momentum in momentum and nesterov gradients.  Default is 0.9.  \n\n\nlr_decay\n - Decrease of the learning rate if the cost function is not decreasing.  Default is 0.9\n\n\nlr_growth\n - Increase of the learning rate if the cost function is decreasing.  Default is 1.0 (i.e., it does not increase.  Adding a small term could, e.g., 1.01, could improve speed).  \n\n\neval_step\n - Number iterations between cost function evaluation in order to determine learning rate decay or growth.  Setting this to a lower number will increase overhead.  Default is 23.  \n\n\nmax_iter\n - Maximum number of iterations.  Default is 10000 \n\n\nstop_threshold\n - Threshold for stopping.  If the cost does not decrease by this proportion, then allow the training to stop.  Default is 0.995.\n\n\npatience\n - Perform at least many iterations before stopping. Default is 2000. \n\n\npatience_incr\n - If a new lowest cost is found, wait at least \npatience_incr * current iteration\n before stopping.  Default is 2.  \n\n\nrand_seed\n - Random seed for initializing model parameters.  Default is 123.", 
            "title": "trainCoxMlp"
        }, 
        {
            "location": "/trainCoxMlp/#traincoxmlp", 
            "text": "This function is the main entry point to building a Cox-nnet survival model.    Parameters:   x_train  - training set matrix.  Expected numpy array.    ytime_train  - time of death or censoring for each patient  ystatus_train  - censoring of each patient  model_params = {}  - model parameters, see below  search_params = {}  - model search/training parameters, see below  verbose=True  - print more stuff.     model_params  is a dictionary of model parameters used in model training.  It has the following parameters:   L2_reg  - Ridge regression parameter for regularization.  Default value is  numpy.exp(-1)  node_map  - mapping of neurons in network  input_split  - splitting of input layer for customization of the network architecture   The network architecture is defined by the  node_map  and  input_split  parameters.  Setting these values to  None  will default to a neural network of one hidden layer with the number of neurons in the hidden layer =  ceil( sqrt( # of input nodes )) .   input_split  is a list of lists with the indices of the inputs determining which sub-input layer each input belongs to.  For example, if you want to split an input with four features into two groups of two:  input_split = [[0,1], [2,3]] .    node_map  defines the connections of the input layer (or sub-input layers).   node_map  list of list of triples (one list for each hidden layer).  A triple defines the inputs of a sub-hidden layer and how many output nodes it has.  I.e.,  (# of input neurons, list of input sub-layers, # of output neurons) .  The  simpleNetArch  function can be used to generate an architecture with a given number of hidden layer neurons.    search params  is a dictionary of search/optimization hyper-parameters.  Generally the default values will work pretty well and you don't need to change these parameters.     method  - The algorithm for gradient descent.  Includes standard gradient descent (\"gradient\"), Nesterov accelerated gradient \"nesterov\" and momentum gradient descent (\"momentum\").  Default is \"nesterov\".    learning_rate  - Initial learning rate.  Default is 0.01  momentum  - proportion of momentum in momentum and nesterov gradients.  Default is 0.9.    lr_decay  - Decrease of the learning rate if the cost function is not decreasing.  Default is 0.9  lr_growth  - Increase of the learning rate if the cost function is decreasing.  Default is 1.0 (i.e., it does not increase.  Adding a small term could, e.g., 1.01, could improve speed).    eval_step  - Number iterations between cost function evaluation in order to determine learning rate decay or growth.  Setting this to a lower number will increase overhead.  Default is 23.    max_iter  - Maximum number of iterations.  Default is 10000   stop_threshold  - Threshold for stopping.  If the cost does not decrease by this proportion, then allow the training to stop.  Default is 0.995.  patience  - Perform at least many iterations before stopping. Default is 2000.   patience_incr  - If a new lowest cost is found, wait at least  patience_incr * current iteration  before stopping.  Default is 2.    rand_seed  - Random seed for initializing model parameters.  Default is 123.", 
            "title": "trainCoxMlp"
        }, 
        {
            "location": "/simpleNetArch/", 
            "text": "simpleNetArch\n\n\nThis function is a helper function for generating the network architecture parameter.  A list object used as input for the \nnode_map\n parameter is returned.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nn_nodes\n - number of nodes desired in the hidden layer.", 
            "title": "simpleNetArch"
        }, 
        {
            "location": "/simpleNetArch/#simplenetarch", 
            "text": "This function is a helper function for generating the network architecture parameter.  A list object used as input for the  node_map  parameter is returned.    Parameters:   x_train  - training set matrix.  Expected numpy array.    n_nodes  - number of nodes desired in the hidden layer.", 
            "title": "simpleNetArch"
        }, 
        {
            "location": "/CVLoglikelihood/", 
            "text": "CVLoglikelihood\n\n\nThis function calculates the cross validated log likelihood score of a validation set.  The score is calculated in the context of the training data by means of subtraction.  See \nHouwelingen et al., 2006\n for more information.  \n\n\nParameters:\n\n\n\n\nmodel\n - the model returned by \ntrainCoxMlp\n\n\nx_full\n - `2D numpy array of the training and validation set\n\n\nytime_full\n - time of death or censoring for each patient in both training and validation set\n\n\nystatus_full\n - censoring of each patient in both training and validation set\n\n\nx_train\n - 2D numpy array of the training set\n\n\nytime_train\n - time of death or censoring for each patient in the training set\n\n\nystatus_train\n - censoring of each patient in the training set\n\n\n\n\nReference: \nvan Houwelingen, Hans C., et al. \"Cross-validated Cox regression on microarray gene expression data.\" Statistics in medicine 25.18 (2006): 3201-3216.", 
            "title": "CVLoglikelihood"
        }, 
        {
            "location": "/CVLoglikelihood/#cvloglikelihood", 
            "text": "This function calculates the cross validated log likelihood score of a validation set.  The score is calculated in the context of the training data by means of subtraction.  See  Houwelingen et al., 2006  for more information.    Parameters:   model  - the model returned by  trainCoxMlp  x_full  - `2D numpy array of the training and validation set  ytime_full  - time of death or censoring for each patient in both training and validation set  ystatus_full  - censoring of each patient in both training and validation set  x_train  - 2D numpy array of the training set  ytime_train  - time of death or censoring for each patient in the training set  ystatus_train  - censoring of each patient in the training set   Reference:  van Houwelingen, Hans C., et al. \"Cross-validated Cox regression on microarray gene expression data.\" Statistics in medicine 25.18 (2006): 3201-3216.", 
            "title": "CVLoglikelihood"
        }, 
        {
            "location": "/CIndex/", 
            "text": "CIndex\n\n\nThis function calculates the Concordance Index (C-Index) of the model.  This statistic is the area under the curve in a classification model.  \n\n\nParameters:\n\n\n\n\nmodel\n - the model returned by \ntrainCoxMlp\n\n\nx_test\n - `2D numpy array of features\n\n\nytime_test\n - time of death or censoring for each patient\n\n\nystatus_test\n - censoring of each patient\n\n\n\n\nReference: \nHeagerty, Patrick J., and Yingye Zheng. \"Survival model predictive accuracy and ROC curves.\" Biometrics 61.1 (2005): 92-105.", 
            "title": "CIndex"
        }, 
        {
            "location": "/CIndex/#cindex", 
            "text": "This function calculates the Concordance Index (C-Index) of the model.  This statistic is the area under the curve in a classification model.    Parameters:   model  - the model returned by  trainCoxMlp  x_test  - `2D numpy array of features  ytime_test  - time of death or censoring for each patient  ystatus_test  - censoring of each patient   Reference:  Heagerty, Patrick J., and Yingye Zheng. \"Survival model predictive accuracy and ROC curves.\" Biometrics 61.1 (2005): 92-105.", 
            "title": "CIndex"
        }, 
        {
            "location": "/L2CVSearch/", 
            "text": "L2CVSearch\n\n\nThis is a function for performing cross validation for optimizing the L2 regularization parameter.  It uses a basic hill climbing algorithm to search a range of parameters.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient\n\n\nmodel_params = {}\n - model parameters, see \ntrainCoxMlp\n\n\nsearch_params = {}\n - model search/training parameters, see \ntrainCoxMlp\n\n\ncv_params\n - cross validation parameters, see below.  \n\n\nverbose=True\n - print more stuff.  \n\n\n\n\ncv_params\n is a dictionary of parameters for cross validation.  It has the following parameters:\n\n\n\n\ncv_seed\n - Random seed for splitting validation folds.  Default is 1.  \n\n\nn_folds\n - Number of folds to cross-validate.  Default is 10.   \n\n\ncv_metric\n - Performance metric for evaluating cross-validation performance.  \nloglikelihood\n or \ncindex\n.  Default is \nloglikelihood\n.  \n\n\nsearch_iters\n - Number of iterations in hill climbing.  Default is 3.  \n\n\nL2_range\n - Range to search for L2 parameter.  Default \n[-5,-1]\n.", 
            "title": "L2CVSearch"
        }, 
        {
            "location": "/L2CVSearch/#l2cvsearch", 
            "text": "This is a function for performing cross validation for optimizing the L2 regularization parameter.  It uses a basic hill climbing algorithm to search a range of parameters.    Parameters:   x_train  - training set matrix.  Expected numpy array.    ytime_train  - time of death or censoring for each patient  ystatus_train  - censoring of each patient  model_params = {}  - model parameters, see  trainCoxMlp  search_params = {}  - model search/training parameters, see  trainCoxMlp  cv_params  - cross validation parameters, see below.    verbose=True  - print more stuff.     cv_params  is a dictionary of parameters for cross validation.  It has the following parameters:   cv_seed  - Random seed for splitting validation folds.  Default is 1.    n_folds  - Number of folds to cross-validate.  Default is 10.     cv_metric  - Performance metric for evaluating cross-validation performance.   loglikelihood  or  cindex .  Default is  loglikelihood .    search_iters  - Number of iterations in hill climbing.  Default is 3.    L2_range  - Range to search for L2 parameter.  Default  [-5,-1] .", 
            "title": "L2CVSearch"
        }, 
        {
            "location": "/L2CVProfile/", 
            "text": "L2CVProfile\n\n\nThis is a function for performing cross validation on a list of given (log) values.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient\n\n\nmodel_params = {}\n - model parameters, see \ntrainCoxMlp\n\n\nsearch_params = {}\n - model search/training parameters, see \ntrainCoxMlp\n\n\ncv_params\n - cross validation parameters, see below.  \n\n\nverbose=True\n - print more stuff.  \n\n\n\n\ncv_params\n is a dictionary of parameters for cross validation.  It has the following parameters:\n\n\n\n\ncv_seed\n - Random seed for splitting validation folds.  Default is 1.  \n\n\nn_folds\n - Number of folds to cross-validate.  Default is 10.   \n\n\ncv_metric\n - Performance metric for evaluating cross-validation performance.  \nloglikelihood\n or \ncindex\n.  Default is \nloglikelihood\n.  \n\n\nsearch_iters\n - Number of iterations in hill climbing.  Default is 3.  \n\n\nL2_range\n - List of(log) L2 parameters to cross-validate.  Default \n[-5,-4,-3,-2,-1]\n.", 
            "title": "L2CVProfile"
        }, 
        {
            "location": "/L2CVProfile/#l2cvprofile", 
            "text": "This is a function for performing cross validation on a list of given (log) values.    Parameters:   x_train  - training set matrix.  Expected numpy array.    ytime_train  - time of death or censoring for each patient  ystatus_train  - censoring of each patient  model_params = {}  - model parameters, see  trainCoxMlp  search_params = {}  - model search/training parameters, see  trainCoxMlp  cv_params  - cross validation parameters, see below.    verbose=True  - print more stuff.     cv_params  is a dictionary of parameters for cross validation.  It has the following parameters:   cv_seed  - Random seed for splitting validation folds.  Default is 1.    n_folds  - Number of folds to cross-validate.  Default is 10.     cv_metric  - Performance metric for evaluating cross-validation performance.   loglikelihood  or  cindex .  Default is  loglikelihood .    search_iters  - Number of iterations in hill climbing.  Default is 3.    L2_range  - List of(log) L2 parameters to cross-validate.  Default  [-5,-4,-3,-2,-1] .", 
            "title": "L2CVProfile"
        }, 
        {
            "location": "/predictNewData/", 
            "text": "trainCoxMlp\n\n\nThis function makes prediction on new data.  The output is the linear predictor (log hazard ratio).  \n\n\nParameters:\n\n\n\n\nmodel\n - model produced by \ntrainCoxMlp\n\n\nx_test\n - Numpy array of features.", 
            "title": "predictNewData"
        }, 
        {
            "location": "/predictNewData/#traincoxmlp", 
            "text": "This function makes prediction on new data.  The output is the linear predictor (log hazard ratio).    Parameters:   model  - model produced by  trainCoxMlp  x_test  - Numpy array of features.", 
            "title": "trainCoxMlp"
        }, 
        {
            "location": "/varImportance/", 
            "text": "varImportance\n\n\nThis function calculates relative variable importance by drop-out.  The values of a variable are set to its mean and the log likelihood is recalculated.  The difference between the original log likelihood and the new log likelihood is the variable importance.  \n\n\nParameters:\n\n\n\n\nmodel\n - the model returned by \ntrainCoxMlp\n \n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient", 
            "title": "varImportance"
        }, 
        {
            "location": "/varImportance/#varimportance", 
            "text": "This function calculates relative variable importance by drop-out.  The values of a variable are set to its mean and the log likelihood is recalculated.  The difference between the original log likelihood and the new log likelihood is the variable importance.    Parameters:   model  - the model returned by  trainCoxMlp    x_train  - training set matrix.  Expected numpy array.    ytime_train  - time of death or censoring for each patient  ystatus_train  - censoring of each patient", 
            "title": "varImportance"
        }, 
        {
            "location": "/saveModel/", 
            "text": "saveModel\n\n\nA helper function for saving a model to a binary file.\n\n\nParameters:\n\n\n\n\nmodel\n - the model returned by \ntrainCoxMlp\n.\n\n\nfile\n - name of file to save to.", 
            "title": "saveModel"
        }, 
        {
            "location": "/saveModel/#savemodel", 
            "text": "A helper function for saving a model to a binary file.  Parameters:   model  - the model returned by  trainCoxMlp .  file  - name of file to save to.", 
            "title": "saveModel"
        }, 
        {
            "location": "/loadModel/", 
            "text": "saveModel\n\n\nA helper function for loading a model from a binary file.\n\n\nParameters:\n\n\n\n\nfile\n - name of file to load from.", 
            "title": "loadModel"
        }, 
        {
            "location": "/loadModel/#savemodel", 
            "text": "A helper function for loading a model from a binary file.  Parameters:   file  - name of file to load from.", 
            "title": "saveModel"
        }, 
        {
            "location": "/example/", 
            "text": "Example usage\n\n\nTODO", 
            "title": "Example"
        }, 
        {
            "location": "/example/#example-usage", 
            "text": "TODO", 
            "title": "Example usage"
        }
    ]
}